# -*- coding: utf-8 -*-
"""proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19A8feJtjkJ2yO-lPV9FzWgaPVehALhrh
"""

import pandas as pd

df = pd.read_csv('d_tweets.csv')

df.columns

df['username'].value_counts()

df2 = pd.read_csv('non_d_tweets.csv')

len(df2['username'].value_counts())

df['tweet']

df = df['tweet']
df2 = df2['tweet']
df = pd.DataFrame({'label' : [1 for i in range(len(df))], 'text': df.values})
df2 = pd.DataFrame({'label' : [0 for i in range(len(df2))], 'text': df2.values})
df

df = pd.concat([df,df2])

df

import re
import nltk
import numpy as np
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

nltk.download(['wordnet', 'stopwords', 'punkt'])

stop=set(stopwords.words('english'))
filter = set(['iyanlavanzant', 'fixmylife', 'iyanla', 'http'])
l = WordNetLemmatizer()
def process(text):
  text=re.sub(r'\s+',' ',text, flags=re.I)
  #remove spl characters other than A-Z,a-z,0-9 or _
  text=re.sub(r'\W',' ',text)

  #remove single character
  text=re.sub(r'\s+[a-zA-Z]\s+',' ',text)

  #remove character not alphabetical
  text=re.sub(r'[^a-zA-Z\s]','',text)

  text=text.lower()

  words= word_tokenize(text)

  words=[l.lemmatize(i) for i in words]
  Words=[word for word in words if word not in stop and word not in filter]


  Words=[word for word in Words if len(word)>3]

  #removing duplicate words
  indices = np.unique(Words, return_index=True)[1]
  cleaned_text = np.array(Words)[np.sort(indices)].tolist()

  return cleaned_text

df['text'] = [process(i) for i in df['text']]
x = df['text']
y = df['label']
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = .2, random_state=42)

import tensorflow as tf
import keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
max_vocab = 20000
t = Tokenizer(num_words=max_vocab)
t.fit_on_texts(x_train)
word_idx = t.word_index
v = len(word_idx)
print('vocab size', v)
x_train = t.texts_to_sequences(x_train)
x_test = t.texts_to_sequences(x_test)

maxlen = 100
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)
x_train

x_test

df['text']

from collections import Counter
import seaborn as sns
import matplotlib.pyplot as plt

all_words = []
for sentence in df[df['label'] == 0]['text'].to_list():
    for word in sentence:
        all_words.append(word)

graph = pd.DataFrame(Counter(all_words).most_common(25), columns= ['Word', 'Frequency'])

sns.set_context('notebook', font_scale= 1.3)
plt.figure(figsize=(18,8))
sns.barplot(y = graph['Word'], x= graph['Frequency'], palette= 'summer')
plt.title("Most Commonly Used Words When Not Depressed")
plt.xlabel("Frequnecy")
plt.ylabel("Words")
plt.show()

all_words = []
for sentence in df[df['label'] == 1]['text'].to_list():
    for word in sentence:
        all_words.append(word)

graph = pd.DataFrame(Counter(all_words).most_common(25), columns= ['Word', 'Frequency'])

sns.set_context('notebook', font_scale= 1.3)
plt.figure(figsize=(18,8))
sns.barplot(y = graph['Word'], x= graph['Frequency'], palette= 'flare')
plt.title("Most Commonly Used Words When Depressed")
plt.xlabel("Frequnecy")
plt.ylabel("Words")
plt.show()

from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from scikitplot.metrics import plot_confusion_matrix, plot_roc

def train_model(model):
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    y_prob = model.predict_proba(x_test)
    accuracy = round(accuracy_score(y_test, y_pred), 3)
    precision = round(precision_score(y_test, y_pred), 3)
    recall = round(recall_score(y_test, y_pred), 3)

    print(f'Accuracy of the model: {accuracy}')
    print(f'Precision Score of the model: {precision}')
    print(f'Recall Score of the model: {recall}')

    sns.set_context('notebook', font_scale= 1.3)
    fig, ax = plt.subplots(1, 2, figsize = (25,  8))
    ax1 = plot_confusion_matrix(y_test, y_pred, ax= ax[0], cmap= 'YlGnBu')
    ax2 = plot_roc(y_test, y_prob, ax= ax[1], plot_macro= False, plot_micro= False, cmap= 'summer')
    return model

nb = MultinomialNB()
nb = train_model(nb)

rf = RandomForestClassifier(n_estimators= 300)
rf = train_model(rf)

sv = SVC(probability=True)
sv = train_model(sv)

!pip install xgboost

from xgboost import XGBClassifier

xgb = XGBClassifier(n_estimators=300)
xgb = train_model(xgb)

from sklearn.ensemble import HistGradientBoostingClassifier
hgb = HistGradientBoostingClassifier(max_iter=300)
train_model(hgb)

from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(n_estimators=300)
train_model(gb)

tad = process('But yeah I think reading through social media posts would be pretty good')
tad = t.texts_to_sequences(tad)
maxlen = 100
tad = pad_sequences(tad, maxlen=maxlen)
rf.predict(tad)

