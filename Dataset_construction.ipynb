{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6S97RCvIg7N7"
      },
      "outputs": [],
      "source": [
        "#https://www.kaggle.com/datasets/hyunkic/twitter-depression-dataset\n",
        "#https://github.com/eddieir/Depression_detection_using_Twitter_post/blob/master/depression.py\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/depressive_tweets_processed.csv\",sep = '|', header = None, usecols = range(0,9))\n",
        "df.columns = [3, \"date\", \"time\", \"time-zone\", \"user\", \"tweet\", 0, 1, 2]\n",
        "df = df.drop([0, 1, 2, 3, \"time-zone\", \"time\", \"date\", \"user\"], axis = 1)\n",
        "df['label'] = [1] * len(df['tweet']) # 1 cause these are all depressed tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2_d = pd.read_csv(\"/content/d_tweets.csv\")\n",
        "df2_d = df2_d[[\"tweet\"]]\n",
        "df2_d['label'] = [1] * len(df2_d['tweet'])\n",
        "\n",
        "df2_nd = pd.read_csv(\"/content/non_d_tweets.csv\")\n",
        "df2_nd = df2_nd[[\"tweet\"]]\n",
        "df2_nd['label'] = [0] * len(df2_nd['tweet'])\n",
        "print(len(df2_nd['label']))\n",
        "df = pd.concat([df2_d, df2_nd, df])\n",
        "df.reset_index(inplace = True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru3OC7C0BpFY",
        "outputId": "6fc9e101-9819-4df8-d784-dd97e745e0b9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiC94VjGDEKJ",
        "outputId": "7e149570-19b1-4388-b93c-c9b43833a733"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "1    5841\n",
              "0    4809\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1utwheNIpzs",
        "outputId": "408ad624-436e-4347-ad76-96d92c5c158d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10650 entries, 0 to 10649\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   index   10650 non-null  int64 \n",
            " 1   tweet   10618 non-null  object\n",
            " 2   label   10650 non-null  int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 249.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(['wordnet', 'stopwords', 'punkt'])\n",
        "df['tweet'] = df['tweet'].astype(str)\n",
        "stop=set(stopwords.words('english'))\n",
        "filter = set(['iyanlavanzant', 'fixmylife', 'iyanla', 'http'])\n",
        "l = WordNetLemmatizer()\n",
        "def process(text):\n",
        "  text=re.sub(r'\\s+',' ',text, flags=re.I)\n",
        "  #remove spl characters other than A-Z,a-z,0-9 or _\n",
        "  text=re.sub(r'\\W',' ',text)\n",
        "\n",
        "  #remove single character\n",
        "  text=re.sub(r'\\s+[a-zA-Z]\\s+',' ',text)\n",
        "\n",
        "  #remove character not alphabetical\n",
        "  text=re.sub(r'[^a-zA-Z\\s]','',text)\n",
        "\n",
        "  text=text.lower()\n",
        "\n",
        "  words= word_tokenize(text)\n",
        "\n",
        "  words=[l.lemmatize(i) for i in words]\n",
        "  Words=[word for word in words if word not in stop and word not in filter]\n",
        "\n",
        "\n",
        "  Words=[word for word in Words if len(word)>2]\n",
        "\n",
        "  #removing duplicate words\n",
        "  indices = np.unique(Words, return_index=True)[1]\n",
        "  cleaned_text = np.array(Words)[np.sort(indices)].tolist()\n",
        "\n",
        "  return cleaned_text\n",
        "\n",
        "df['tweet'] = [process(i) for i in df['tweet']]\n",
        "x = df['tweet']\n",
        "y = df['label']\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = .2, random_state=42)\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_vocab = 20000\n",
        "t = Tokenizer(num_words=max_vocab)\n",
        "t.fit_on_texts(x_train)\n",
        "word_idx = t.word_index\n",
        "v = len(word_idx)\n",
        "print('vocab size', v)\n",
        "x_train = t.texts_to_sequences(x_train)\n",
        "x_test = t.texts_to_sequences(x_test)\n",
        "\n",
        "maxlen = 100\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "x_train\n",
        "\n",
        "x_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQOXUH0aH3vu",
        "outputId": "b080f247-ede5-42c1-9309-6ce4846ce51d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size 14387\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,  730,  326,  126],\n",
              "       [   0,    0,    0, ...,  606,  366,    4],\n",
              "       [   0,    0,    0, ...,  495,  659,  353],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,  162,  164,  187],\n",
              "       [   0,    0,    0, ...,  278, 1465,  302],\n",
              "       [   0,    0,    0, ...,    0,    0,    1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}